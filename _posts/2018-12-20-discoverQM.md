---
layout: post
title: Rediscover Quantum Mechanics in Machine Learning
tags: [Quantum Mechanics, Machine Learning, Recurrent Neural Network]
---

Emergent phenomenon is a central theme of condensed matter physics. Not only matter and forces can be emergent, spacetime and gravity could also be emergent. These interesting ideas are been actively explored in the frontier of physics research. But, wait a moment, aren't all these physics theories themselves also emergent phenomena? This is an interesting idea. Physics theories are indeed collective neural activations in human brain. It is  unclear how the ideas of physics emerge in the neural network of a physicist, or more generally, in the physics community. Understanding the universal principles of emergent intelligence in complex networks should be one important topic of science.

We are probably still far away from a full understanding of intelligence. But the recent development in machine learning allow us to make a first step towards this direction. We would like to investigate whether artificial neural networks can be used to discover physical concepts and laws from experimental data. Let us take quantum mechanics for example. Suppose quantum mechanics has not be formulated so far, yet amazingly, physicists somehow knows how to perform cold atom experiments to collect density distributions of Bose-Einstein condensate (BEC) in potential traps of different shapes. Can quantum mechanics be discovered as the most natural theory to explain the experimental data without the prior bias of human physics?

In our recent work, we demonstrate how a machine learning algorithm can discover quantum mechanics in learning to predict the BEC density given the potential profile. The machine is only exposed to the data of potential and density, yet the quantum wave function can emergent as latent variables in the neural network. We are inspired by the development of machine translation, which is trained to map sequences of words from one language to another. The machine translator is able to develop a semantic space in its hidden layers, which holds the intrinsic representation of words or phrases that are universal to all languages. By analyzing the structure of the semantic space, we could possibly gain understanding about the relations among words as perceived by the translator. To put our problem in this context, we treat the potential-to-density mapping as an example of sequence-to-sequence mapping which can be handled by machine translation approach, such as recurrent neural network.

![Introspective recurrent neural network]({{site.baseurl}}/assets/img/figures/introspective_RNN.png)

We train a recurrent neural network to translate the potential profile to the density profile along a one-dimensional trap. By learning to perform this translation, the machine must have also gain intuitions about the underlying physics. To extract what has been realized by the machine translator, we design a higher-level machine, called knowledge distiller, to learn from the neural activations (hidden states) of lower-level translator. The knowledge distiller is an auto-encoder incorporated in another recurrent neural network structure. Its task is compress the hidden states generated by the translator at each step as much as possible, without loosing the prediction power to reconstruct subsequent hidden states. In this way, the knowledge distiller can identify the most essential variables. Our study shows that the reconstruction loss of the knowledge distiller only increase abruptly when its latent space dimension is reduced below two. This implies that at least two real variables are required to describe the behavior of the potential-to-density translator. As we plot these two variables out, they correspond to the real and imaginary parts of the quantum wave function (up to basis freedom). Further inspection of the update rules for these variables shows that they are governed by a recurrent relation which precisely matches the discrete version of the Schr√∂dinger equation. Knowledge about quantum mechanics indeed emerges in the neural network.



