---
layout: post
title: Rediscover Quantum Mechanics in Machine Learning
tags: [Quantum Mechanics, Machine Learning, Recurrent Neural Network]
---

The emergent phenomenon is a central theme of condensed matter physics. Not only matter and forces can be emergent, spacetime and gravity could also be emergent. These exciting ideas are being actively explored in the frontier of physics research. But wait a moment, aren't all these physics theories themselves also emergent phenomena? This is an interesting idea. Physics theories are indeed collective neural activations in human's brain. It is unclear how the ideas of physics emerge in the neural network of a physicist, or more generally, in the physics community. Understanding the universal principles of emergent intelligence in complex networks should be one important goal of science.

We are probably still far away from a full understanding of intelligence. However, the recent development in machine learning allows us to make the first step towards this direction. We want to investigate whether artificial neural networks can be used to discover physical concepts and laws from experimental data. Let us take quantum mechanics for example. Suppose quantum mechanics has not been formulated so far, yet amazingly, physicists somehow know how to perform cold atom experiments to collect density distributions of Bose-Einstein condensate (BEC) in potential traps of different shapes. Can quantum mechanics be discovered as the most natural theory to explain the experimental data without the prior bias of human civilization? Or will the machine come up with an alternative form of quantum mechanics?

In our recent work [arXiv:1901.11103](https://arxiv.org/abs/1901.11103), we demonstrate how a machine learning algorithm can discover quantum mechanics in learning to predict the BEC density given the potential profile. The machine is only exposed to the data of potentials and densities, yet the quantum wave function can emergent as latent variables in the neural network. We are inspired by the development of machine translation, which is trained to map sequences of words from one language to another. The machine translator can develop a semantic space in its hidden layers, which holds the intrinsic representation of words or phrases that are universal to all languages. By analyzing the structure of the semantic space, we could gain understanding about the relations among words as perceived by the translator. To put our problem in this context, we treat the potential-to-density mapping as an example of sequence-to-sequence mapping which can be handled by the machine translation approach, such as the recurrent neural network.

![Introspective recurrent neural network]({{site.baseurl}}/assets/img/figures/introspective_RNN.png)

We train a recurrent neural network to translate the potential profile to the density profile along a one-dimensional trap. By learning to perform this translation, the machine must have also gain intuitions about the underlying physics. To extract what has been realized by the machine translator, we design a higher-level machine, called knowledge distiller, to learn from the neural activations (hidden states) of the lower-level translator. The knowledge distiller is an auto-encoder incorporated in another recurrent neural network structure. Its task is to compress the hidden states generated by the translator at each step as much as possible, without losing the prediction power to reconstruct subsequent hidden states. In this way, the knowledge distiller can identify the essential variables. Our study shows that the reconstruction loss of the knowledge distiller only increase abruptly when its latent space dimension is reduced below two. This implies that at least two real variables are required to describe the behavior of the potential-to-density translator. As we plot these two variables out, they correspond to the real and imaginary parts of the quantum wave function (up to basis freedom). Further inspection of the update rules for these variables shows that they are governed by a recurrent relation which precisely matches the discrete version of the Schrödinger equation. Knowledge about quantum mechanics indeed emerges in the neural network.

Moreover, if we relax the information bottleneck of the knowledge distiller, alternative forms of quantum mechanics, such as the density functional theory, could also emerge, but it requires at least three real variables to describe. It could be reassuring to know that human's current formulation of quantum mechanics, in terms of the wave function and Schrödinger equation, is indeed the most parsimonious theory among all alternative theories of quantum mechanics that have been discovered in our neural network.
